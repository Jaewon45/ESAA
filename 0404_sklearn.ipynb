{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d685815",
   "metadata": {},
   "source": [
    "# Chapter 2 . 사이킷런으로 시작하는 머신러닝\n",
    "\n",
    "## 01. 사이킷런 소개와 특징\n",
    "- 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향한만큼 쉽고 파이썬스러운 API 제공\n",
    "- 머신러닝을 위한 매우 다양한 알고리즘, 개발을 위한 편리한 프레임워크/API 제공\n",
    "- 오랜 기간 실전 환경에서 검증, 매우 많은 환경에서 사용됨\n",
    "\n",
    "## 02. 첫번째 머신러닝 만들어보기 - 붓꽃 품종 예측하기\n",
    "- 붓꽃의 품종 분류 \n",
    "- 분류 : 대표적 지도학습 (학습을 위한 다양한 피처와 레이블 데이터로 모델을 학습 후 별도의 테스트 데이터 세트에서 미지의 레이블 예측)\n",
    " - 사이킷런 패키지 내 모듈명 sklearn으로 시작 \n",
    " - sklearn.model_selection : 학습 데이터 / 검증 데이터 / 예측데이터로 분리 OR 최적의 하이퍼파라미터로 평가하기 위한 모듈 모임\n",
    " - 하이퍼 파라미터 : 머신러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터 통칭. 머신러닝 알고리즘 성능 튜닝 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae543b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 데이터 세트 준비\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "iris_df.head(3)\n",
    "\n",
    "# 의사결정 트리를 이요한 학습과 예측 수행\n",
    "# 1) 학습용 데이터 / 테스트용 데이터 세트 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)\n",
    "\n",
    "# 의사결정 트리 클래스 객체로 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "\n",
    "# 2) 모델 학습 \n",
    "dt_clf.fit(X_train,y_train)\n",
    "\n",
    "# 3) 예측 수행\n",
    "# 테스트 데이터 세트로 수행, 학습데이터 사용하면 안됨\n",
    "pred = dt_clf.predict(X_test)\n",
    "\n",
    "# 4) 예측 성능 평가 - 정확도\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012760e3",
   "metadata": {},
   "source": [
    "## 03. 사이킷런의 기반 프레임워크 익히기\n",
    "\n",
    "### (1) Estimator 이해 및 fit(), predict() 메서드  \n",
    "#### a) 지도학습 (분류, 회귀)\n",
    "- fit() : 모델 학습\n",
    "- predict() : 학습된 모델의 예측\n",
    "- estimator : 지도학습의 모든 알고리즘을 구현한 클래스 통칭 (Classifier / Regressor). 파라미터 튜닝을 지원하는 클래스 및 evaluation 함수의 인자로 사용\n",
    "\n",
    "#### b) 비지도학습\n",
    "- 차원축소, 클러스터링, 피처 추출\n",
    "- fit() : 입력형태에 맞춰 데이터를 변환하기 위한 사전 구조를 맞추는 작업\n",
    "- transform() : 사전 구조를 맞춘 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출 등의 실제 작업 수행\n",
    "- fit_transform()을 통해 한번에 사용 가능\n",
    "\n",
    "### (2) 사이킷런의 주요 모듈\n",
    "- sklearn.datasets\n",
    "- sklearn.preprocessing, sklearn.feature_selection\n",
    "- sklearn.feature_extraction : 피처 처리 (추출)\n",
    "- sklearn.decomposition : 차원 축소\n",
    "- sklearn.model_selection : 교차검증위한 분리, 그리드 서치로 최적 파라미터 추출\n",
    "- sklearn.metrics : 분류, 회귀, 클러스터링, 페어와이즈에 대한 성능측정 (정확도, RMSE 등)\n",
    "- ML 알고리즘 모듈 : sklearn.ensemble, sklearn.linear_model, sklearn.naive_bayes, sklearn.neighbors, sklearn.svm, sklearn.tree, sklearn.cluster\n",
    "- sklearn.pipeline\n",
    "\n",
    "### (3) 내장된 예제 데이터 세트\n",
    "- 분류, 회귀 연습용 예제 데이터\n",
    " - datasets.load_boston(), datasets.load_breast_cancer(), datasets.load_diabetes(), datasets.load_digits(), datasets.load_iris()\n",
    " - 일반적으로 딕셔너리 형태\n",
    " - 키 : data(ndarray), target(ndarray), target_name(list or ndarray), feature_names(list or ndarray)/DESCR(str)\n",
    "- fetch 계열 명령 데이터 (최초 사용시 인터넷에 연결)\n",
    " - fetch_covtype(), fetch_20newsgroups(), fetch_olivetti_faces(), fetch_lfw_people(), fetch_lfw_pairs(), fetch_rcv1, fetch_mldata()\n",
    "- 분류, 클러스터링 용 표본데이터 생성기\n",
    " - datasets.make_classifications(), datasets.make_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e22ded1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "\n",
      " feature_names의 type: <class 'list'>\n",
      "feature_names의 shape: 4\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      " target_names의 type: <class 'numpy.ndarray'>\n",
      "target_names의 shape: 3\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      " data의 type: <class 'numpy.ndarray'>\n",
      "data의 shape: (150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "\n",
      " target의 type: <class 'numpy.ndarray'>\n",
      "target의 shape: (150,)\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data=load_iris()\n",
    "print(type(iris_data))\n",
    "print(iris_data.keys())\n",
    "\n",
    "# 객체 키 출력\n",
    "print('\\n feature_names의 type:', type(iris_data.feature_names))\n",
    "print('feature_names의 shape:', len(iris_data.feature_names))\n",
    "print(iris_data.feature_names)\n",
    "\n",
    "print('\\n target_names의 type:', type(iris_data.target_names))\n",
    "print('target_names의 shape:', len(iris_data.target_names))\n",
    "print(iris_data.target_names)\n",
    "\n",
    "print('\\n data의 type:', type(iris_data.data))\n",
    "print('data의 shape:', iris_data.data.shape)\n",
    "print(iris_data.data[0:3])\n",
    "\n",
    "print('\\n target의 type:', type(iris_data.target))\n",
    "print('target의 shape:', iris_data.target.shape)\n",
    "print(iris_data.target[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88d8ce",
   "metadata": {},
   "source": [
    "## 04. Model Selection 모듈 소개\n",
    "\n",
    "### (1) 학습/테스트 데이터 세트 분리 - train_test_split()\n",
    "- test_size : 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가\n",
    "- train_size : 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가\n",
    "- shuffle : 데이터를 분리하기 전 데이터를 미리 섞을지 결정 (default=True)\n",
    "- random_state\n",
    "- 튜플 형태로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f40d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris=load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "train_data = iris.data\n",
    "train_label = iris.target\n",
    "dt_clf.fit(train_data, train_label)\n",
    "\n",
    "# 학습 데이터 세트로 예측 수행\n",
    "pred = dt_clf.predict(train_data)\n",
    "print('예측 정확도:', accuracy_score(train_label,pred))\n",
    "# 이미 학습한 데이터 세트를 기반으로 예측시 예측 정확도가 100%가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367630c",
   "metadata": {},
   "source": [
    "- 테스트 데이터 세트를 전체의 30%, 학습 데이터 세트를 70%로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ef6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "iris_data = load_iris()\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121)\n",
    "\n",
    "# 학습 데이터 학습 수행 후 모델의 정확도 측정\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "print('예측 정확도:', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb5d7f",
   "metadata": {},
   "source": [
    "### (2) 교차 검증\n",
    "- 모델이 학습데이터에만 과도하게 최적화되는 과적합 발생 위험\n",
    "- 성능 평가시 테스트 데이터에만 최적의 성능을 발휘하도록 편향되게 모델을 유도할 위험\n",
    "- 교차검증의 필요성 \n",
    " - 별도의 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행\n",
    " - 각 세트에서 수행한 평가 결과에 따라 하이퍼 파라미터 튜닝 등의 모델 최적화 손쉽게 진행\n",
    " \n",
    "#### a) K 폴드 교차 검증\n",
    "- K개의 데이터 폴드 세트를 만들어 각 폴드 세트에 학습과 검증 평가 반복적으로 수행\n",
    "- 학습 데이터 세트와 검증 데이터 세트를 점진적으로 변경하면서 마지막 K번째까지 학습과 검증 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809a10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "붓꽃 데이터 세트 크기: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_accuracy = []\n",
    "print('붓꽃 데이터 세트 크기:', features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f182c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도:1.0, 학습데이터 크기: 120, 검증데이터 크기: 30\n",
      "#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "#2 교차 검증 정확도:0.9667, 학습데이터 크기: 120, 검증데이터 크기: 30\n",
      "#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "\n",
      "#3 교차 검증 정확도:0.8667, 학습데이터 크기: 120, 검증데이터 크기: 30\n",
      "#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "\n",
      "#4 교차 검증 정확도:0.9333, 학습데이터 크기: 120, 검증데이터 크기: 30\n",
      "#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#5 교차 검증 정확도:0.7333, 학습데이터 크기: 120, 검증데이터 크기: 30\n",
      "#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "# KFold 객체의 split()를 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환\n",
    "for train_index, test_index, in kfold.split(features):\n",
    "    # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    # 학습 및 예측\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    # 반복 시마다 정확도 측정\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도:{1}, 학습데이터 크기: {2}, 검증데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
    "    cv_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeddd2",
   "metadata": {},
   "source": [
    "#### b) Stratified K 폴드\n",
    "- 불균형한 데이터 분포도를 가진 레이블 데이터 집합을 위한 K 폴드 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209188e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris=load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['label']=iris.target\n",
    "iris_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7592092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차 검증: 1\n",
      "학습 레이블 데이터 분포:\n",
      " 1    50\n",
      "2    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    50\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증: 2\n",
      "학습 레이블 데이터 분포:\n",
      " 0    50\n",
      "2    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 1    50\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증: 3\n",
      "학습 레이블 데이터 분포:\n",
      " 0    50\n",
      "1    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 2    50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3)\n",
    "n_iter = 0\n",
    "for train_index, test_index, in kfold.split(iris_df):\n",
    "    n_iter += 1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print('## 교차 검증: {0}'.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n',label_test.value_counts())    \n",
    "# 데이터 불균형하게 분포, 학습 데이터에 있는 레이블만 예측 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eec29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차 검증: 1\n",
      "학습 레이블 데이터 분포:\n",
      " 2    34\n",
      "0    33\n",
      "1    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    17\n",
      "1    17\n",
      "2    16\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증: 2\n",
      "학습 레이블 데이터 분포:\n",
      " 1    34\n",
      "0    33\n",
      "2    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    17\n",
      "2    17\n",
      "1    16\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증: 3\n",
      "학습 레이블 데이터 분포:\n",
      " 0    34\n",
      "1    33\n",
      "2    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 1    17\n",
      "2    17\n",
      "0    16\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index, in skf.split(iris_df, iris_df['label']):\n",
    "    n_iter += 1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print('## 교차 검증: {0}'.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n',label_test.value_counts())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054e3c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도:0.98, 학습데이터 크기: 100, 검증데이터 크기: 50\n",
      "#1 검증 세트 인덱스:[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "\n",
      "#2 교차 검증 정확도:0.94, 학습데이터 크기: 100, 검증데이터 크기: 50\n",
      "#2 검증 세트 인덱스:[ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n",
      "\n",
      "#3 교차 검증 정확도:0.98, 학습데이터 크기: 100, 검증데이터 크기: 50\n",
      "#3 검증 세트 인덱스:[ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84\n",
      "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "## 평균 검증 정확도: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "skfold = StratifiedKFold(n_splits=3)\n",
    "n_iter=0\n",
    "cv_accuracy=[]\n",
    "\n",
    "\n",
    "# StratifiedKFold의 split 호출시 반드시 레이블 데이터 세트를 추가입력해야함\n",
    "for train_index, test_index, in skfold.split(features, label):\n",
    "    # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    # 학습 및 예측\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    # 반복 시마다 정확도 측정\n",
    "    n_iter += 1\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도:{1}, 학습데이터 크기: {2}, 검증데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "    \n",
    "# 교차 검증별 정확도 및 평균 정확도 계산\n",
    "print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy,4))\n",
    "print('## 평균 검증 정확도:', np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ce826",
   "metadata": {},
   "source": [
    "#### c) 교차 검증을 보다 간편하게 - cross_val_score()\n",
    "- 다음과 같은 과정을 한꺼번에 수행해주는 API\n",
    " - 폴드 세트 설정\n",
    " - for 루프에서 반복적으로 학습 및 테스트데이터의 인덱스 추출\n",
    " - 반복적으로 학습과 예측 수행 후 예측 성능을 반환\n",
    "- cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n",
    " - scoring : 예측성능평가지표\n",
    " - cv : 교차 검증 폴드 수\n",
    " - 반환 : 지정된 성능 지표 측정값을 배열 형태로 반환\n",
    "- classifier가 입력되면 stratified K폴드 방식으로 레이블값 분포에 따라 데이터 세트 분할\n",
    " - 회귀인 경우는 K 폴드 방식으로 분할 \n",
    " \n",
    "#### +) cross_validate() \n",
    "- cross_val_score과 비슷한 API\n",
    "- 여러개의 평가 지표 반환\n",
    "- 학습 데이터에 대한 성능 평가 지표와 수행 시간 또한 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7800b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "평균 검증 정확도: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "data = iris_data.data\n",
    "label = iris_data.target\n",
    "\n",
    "# 성능 지표는 정확도 (accuracy), 교차 검증 세트는 3개\n",
    "scores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)\n",
    "print(\"교차 검증별 정확도:\", np.round(scores,4))\n",
    "print(\"평균 검증 정확도:\", np.round(np.mean(scores),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509f664",
   "metadata": {},
   "source": [
    "### (3) GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한번에\n",
    "- 하이퍼 파라미터 : 머신러닝 알고리즘의 주요 구성 요소, 값을 조정해 ML 성능 개선 가능\n",
    "- GridSearchCV API 이용해 classifier/regressior 등에 사용되는 하이퍼파라미터 순차적으로 입력하며 최적의 파라미터 도출\n",
    "- 순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸림\n",
    "\n",
    "- 주요 파라미터\n",
    " - estimator : classifier / regressor / pipeline\n",
    " - param_grid : 딕셔너리형. estimator의 튜닝을 위한 파라미터명과 파라미터값 지정\n",
    " - scoring : 예측성능 측정 평가방법 지정 (문자열 혹은 함수 지정)\n",
    " - cv : 교차 검증을 위해 분할되는 학습/테스트 세트의 개수\n",
    " - refit : default=True, True시 최적의 하이퍼파라미터를 찾은 뒤입력된 estimator객체를 해당 하이퍼파라미터로 재학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec232d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     params  mean_test_score  rank_test_score  \\\n",
       "0  {'max_depth': 1, 'min_samples_split': 2}         0.700000                5   \n",
       "1  {'max_depth': 1, 'min_samples_split': 3}         0.700000                5   \n",
       "2  {'max_depth': 2, 'min_samples_split': 2}         0.958333                3   \n",
       "3  {'max_depth': 2, 'min_samples_split': 3}         0.958333                3   \n",
       "4  {'max_depth': 3, 'min_samples_split': 2}         0.975000                1   \n",
       "5  {'max_depth': 3, 'min_samples_split': 3}         0.975000                1   \n",
       "\n",
       "   split0_test_score  split1_test_score  \n",
       "0              0.700                0.7  \n",
       "1              0.700                0.7  \n",
       "2              0.925                1.0  \n",
       "3              0.925                1.0  \n",
       "4              0.975                1.0  \n",
       "5              0.975                1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1) 데이터를 로딩하고 학습데이터와 테스트데이터 분리\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=121)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# 파라미터를 딕셔너리 형태로 설정\n",
    "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}\n",
    "# decision tree classifier의 중요 하이퍼파라미터\n",
    "\n",
    "import pandas as pd\n",
    "# param_grid의 하이퍼파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정, refit=True\n",
    "grid_dtree = GridSearchCV(dtree,param_grid=parameters,cv=3, refit=True)\n",
    "\n",
    "# 붓꽃 학습 데이터로 param_grid의 하이퍼파라미터를 순차적으로 학습/평가\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# GradSearchCV 결과를 추출해 dataframe으로 변환\n",
    "scores_df=pd.DataFrame(grid_dtree.cv_results_) # cv_result_ : gridsearchcv의 결과세트\n",
    "scores_df[['params', 'mean_test_score','rank_test_score','split0_test_score', 'split1_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0da43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최적 파라미터: {'max_depth': 3, 'min_samples_split': 2}\n",
      "GridSearchCV 최고 정확도: 0.9750\n"
     ]
    }
   ],
   "source": [
    "print(\"GridSearchCV 최적 파라미터:\", grid_dtree.best_params_)\n",
    "print(\"GridSearchCV 최고 정확도: {0:.4f}\".format(grid_dtree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4dd30f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 세트 정확도: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV의 refit으로 이미 학습된 estimator 반환\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "# 이미 최적의 학습이 되었으므로 별도의 학습은 필요 없고 바로 예측이 가능\n",
    "pred = estimator.predict(X_test)\n",
    "print(\"테스트 데이터 세트 정확도: {0:.4f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abee81",
   "metadata": {},
   "source": [
    "## 05. 데이터 전처리 \n",
    "\n",
    "- 결손값 (NaN, Null 값) 고정된 다른 값으로 변환\n",
    " - 피처의 평균값 등으로 대체, Null 값이 대부분이라면 드롭\n",
    "- 모든 문자열 값 인코딩해 숫자형으로 변환\n",
    " - 카테고리형 피처 : 코드값으로 표현\n",
    " - 텍스트형 피처 : 피처 벡터화, 불필요한 피처인 경우 삭제\n",
    "\n",
    "### (1) 데이터 인코딩\n",
    "#### a) 레이블 인코딩\n",
    "- 카테고리 피처를 코드형 숫자 값으로 변환\n",
    "- 간단하게 문자열 값을 숫자형 카테고리 값으로 변환\n",
    "- 숫자 크기의 특성이 작용해 알고리즘에 적용할 경우 예측 성능이 떨어지는 경우 발생 가능\n",
    " - 선형 회귀와 같은 ML 알고리즘에 적용하지 않아야 함\n",
    " - 트리 계열의 ML 알고리즘에는 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb4ecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 변환값: [0 1 4 5 3 3 2 2]\n",
      "인코딩 클래스: ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']\n",
      "디코딩 원본값: ['전자레인지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "items = ['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "# LabelEncoder를 객체로 생성 후 fit(), transform()으로 인코딩\n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print(\"인코딩 변환값:\", labels)\n",
    "print(\"인코딩 클래스:\", encoder.classes_) # 고유한 값들만 반환\n",
    "print(\"디코딩 원본값:\", encoder.inverse_transform([4,5,2,0,1,1,3,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ecd6b",
   "metadata": {},
   "source": [
    "#### b) 원-핫 인코딩\n",
    "- 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시\n",
    "- 사이킷런 OneHotEncoder 클래스 사용\n",
    " - 모든 문자열 값이 숫자형 값으로 변환돼야 함\n",
    " - 입력값으로 2차원 데이터가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2601c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원-핫 인코딩 데이터\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "원-핫 인코딩 데이터 차원\n",
      "(8, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 숫자 값으로 변환 위해 LabelEncoder로 변환\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "# 2차원 데이터로 변환\n",
    "labels = labels.reshape(-1,1)\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "\n",
    "print(\"원-핫 인코딩 데이터\")\n",
    "print(oh_labels.toarray())\n",
    "print(\"원-핫 인코딩 데이터 차원\")\n",
    "print(oh_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d66fa995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_TV</th>\n",
       "      <th>item_냉장고</th>\n",
       "      <th>item_믹서</th>\n",
       "      <th>item_선풍기</th>\n",
       "      <th>item_전자레인지</th>\n",
       "      <th>item_컴퓨터</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_TV  item_냉장고  item_믹서  item_선풍기  item_전자레인지  item_컴퓨터\n",
       "0        1         0        0         0           0         0\n",
       "1        0         1        0         0           0         0\n",
       "2        0         0        0         0           1         0\n",
       "3        0         0        0         0           0         1\n",
       "4        0         0        0         1           0         0\n",
       "5        0         0        0         1           0         0\n",
       "6        0         0        1         0           0         0\n",
       "7        0         0        1         0           0         0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 판다스에서의 원-핫 인코딩 : 숫자형으로 변환 없이도 적용 가능\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']})\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391a734",
   "metadata": {},
   "source": [
    "### (2) 피처 스케일링과 정규화\n",
    "- 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업\n",
    "- 표준화 : 데이터 피처 각각이 평균 0, 분산 1인 가우시안 정규분포 값으로 변환\n",
    "- 정규화 : 서로 다른 피처의 크기를 통일, 개별 데이터의 크기를 모두 똑같은 단위로 변경\n",
    " - 사이킷런 전처리에서의 Normalizer 모듈은 선형대수에서의 정규화 개념을 적용해 개별 벡터의 크기를 맞춤\n",
    " \n",
    "#### a) StandardScaler\n",
    "- 표준화 지원 클래스\n",
    "- SVM, 선형 회귀, 로지스틱 회귀 데이터는 가우시안 분포 가정, 사전에 표준화 적용이 예측 성능 향상에 도움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eaef83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "sepal length (cm)    0.685694\n",
      "sepal width (cm)     0.189979\n",
      "petal length (cm)    3.116278\n",
      "petal width (cm)     0.581006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(iris_data, columns=iris.feature_names)\n",
    "\n",
    "print(\"feature 들의 평균 값\")\n",
    "print(iris_df.mean())\n",
    "print(\"\\nfeature 들의 분산 값\")\n",
    "print(iris_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e249db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler() \n",
    "# StandardScaler로 데이터 세트를 변환, fit()과 transform() 호출\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df) \n",
    "\n",
    "# transform()시 ndarray로 반환된 데이터 세트를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(iris_scaled, columns=iris.feature_names)\n",
    "print(\"feature 들의 평균 값\")\n",
    "print(iris_df_scaled.mean())\n",
    "print(\"\\nfeature 들의 분산 값\")\n",
    "print(iris_df_scaled.var())\n",
    "\n",
    "# 평균은 0, 분산은 1에 근사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34b165",
   "metadata": {},
   "source": [
    "#### b) MinMaxScaler\n",
    "- 데이터값을 0과 1 사이의 범위값으로 변환 (음수값 존재시 -1에서 1 사이 범위값)\n",
    "- 가우시안 분포가 아닌 데이터에 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4860b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 최솟값\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 최댓값\n",
      "sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1) MinMaxScaler 객체 생성\n",
    "scaler = MinMaxScaler() \n",
    "\n",
    "# 2) MinMaxScaler로 데이터 세트를 변환, fit()과 transform()호출\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df) \n",
    "\n",
    "# 3) transform()시 ndarray로 반환된 데이터 세트를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(iris_scaled, columns=iris.feature_names)\n",
    "print(\"feature 들의 최솟값\")\n",
    "print(iris_df_scaled.min())\n",
    "print(\"\\nfeature 들의 최댓값\")\n",
    "print(iris_df_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d76b2",
   "metadata": {},
   "source": [
    "#### c) 학습 데이터나 테스트 데이터의 스케일링 변환 시 유의점\n",
    "- Scaler 객체를 이용해 학습 데이터 세트로 fit과 transform을 적용하면 테스트 데이터세트로는 다시 fit()을 수행하지 않고 학습 데이터 세트로 fit()을 수행한 결과를 이용해 transform() 변환을 적용해야 함\n",
    "- 테스트데이터로 다시 새로운 스케일링 기준 정보를 만들게 되면 올바른 예측/결과도출 불가\n",
    " - 서로 다른 원본값이 동일한 값으로 변환되는 결과 초래\n",
    " - 반드시 테스트 데이터는 학습 데이터의 스케일링 기준에 따라야 함\n",
    "- fit_transform()을 적용하는 경우\n",
    " - 해당 메소드는 fit()과 transform()을 순차적으로 수행하기 때문에 학습데이터는 상관 없지만 테스트 데이터에서는 절대 사용해서는 안됨\n",
    " - 학습과 데이터 세트로 분리하기 전에 전체 데이터 세트에 스케일링을 적용한 뒤 학습과 테스트 ㅅ데이터 센트로 분리하는 것이 바람직함\n",
    " - 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습데이터로 이미 fit()된 Scaler 객체를 이용해 transform()으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d12ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 train_array 데이터: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Scale된 train_array 데이터: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성\n",
    "# Scaler의 fit(), transform()은 2차원 이상 데이터만 가능하므로 차원을 변경\n",
    "train_array = np.arange(0, 11).reshape(-1,1)\n",
    "test_array = np.arange(0, 6).reshape(-1,1)\n",
    "\n",
    "# MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1값으로 변환\n",
    "scaler = MinMaxScaler() \n",
    "\n",
    "# fit() 하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정됨\n",
    "scaler.fit(train_array)\n",
    "\n",
    "# 1/10 scale로 train_array 데이터를 변환하여 원본 10 -> 1로 변환됨\n",
    "train_scaled = scaler.transform(train_array)\n",
    "\n",
    "print(\"원본 train_array 데이터:\", np.round(train_array.reshape(-1),2))\n",
    "print(\"Scale된 train_array 데이터:\", np.round(train_scaled.reshape(-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80837d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 test_array 데이터: [0 1 2 3 4 5]\n",
      "Scale된 test_array 데이터: [0.  0.2 0.4 0.6 0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 세트 변환, fit()을 호출해 스케일링 기준 정보 적용한 뒤 transform()결과\n",
    "\n",
    "scaler.fit(test_array)\n",
    "# 1/5 scale로 test_array 데이터를 변환하여 원본 5 -> 1로 변환\n",
    "test_scaled = scaler.transform(test_array)\n",
    "\n",
    "print(\"원본 test_array 데이터:\", np.round(test_array.reshape(-1),2))\n",
    "print(\"Scale된 test_array 데이터:\", np.round(test_scaled.reshape(-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09b58817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 train_array 데이터: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Scale된 train_array 데이터: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "원본 test_array 데이터: [0 1 2 3 4 5]\n",
      "Scale된 test_array 데이터: [0.  0.1 0.2 0.3 0.4 0.5]\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로 fit()을 수행한 객체의 transform()을 이용해 테스트 데이터 변환\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_array)\n",
    "train_scaled = scaler.transform(train_array)\n",
    "print(\"원본 train_array 데이터:\", np.round(train_array.reshape(-1),2))\n",
    "print(\"Scale된 train_array 데이터:\", np.round(train_scaled.reshape(-1),2))\n",
    "\n",
    "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 바로 transform()만으로 변환해야 함\n",
    "test_scaled = scaler.transform(test_array)\n",
    "print(\"\\n원본 test_array 데이터:\", np.round(test_array.reshape(-1),2))\n",
    "print(\"Scale된 test_array 데이터:\", np.round(test_scaled.reshape(-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32675e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
