{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaewon45/ESAA_2022/blob/main/0919_LinearModel_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10번\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사하강법은 특성 수에 민감하지 않아 수십만 개의 특성에서 선형 회귀를 훈련시킬 때 SVD 분해보다 훨씬 빠름\n",
        "- 정규방정식을 사용하는 SVD는 예측 계산 복잡도는 샘플 수와 특성 수에 선형적으로 비례하기 때문에 특성 수가 많으면 계산 복잡도가 크게 증가함"
      ],
      "metadata": {
        "id": "QqI0KGanHyRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 훈련오차도 같이 높아지는 경우 적은 수의 샘플로 훈련되는 등의 이유로 모델이 일반화되지 못하는 과소적합 \n",
        "- 훈련오차는 높아지지 않는다면 훈련 세트에 대해 과대적합, 학습률이 너무 높고 점진적으로 감소되지 않는 등의 이유로 발생하는 과대적합, 학습을 조기 종료해야함\n",
        "- 학습률이 너무 높은 경우 알고리즘이 발산하기 때문에 낮추는 것으로 해결할 수 있음"
      ],
      "metadata": {
        "id": "SF7ufA6VJWGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 훈련, 검증오차가 모두 높은 경우 과소적합, 높은 편향이 문제\n",
        "- 하이퍼파라미터 α는 모델을 얼마나 많이 규제할지 조절, 클수록 가중치가 작아짐\n",
        "- 가중치가 작아진다는 것은 곧 모델의 복잡도가 낮아진다는 것을 의미하므로 이는 과대 적합을 피하는 방향으로 학습이 된다는 것을 의미\n",
        "- 따라서 규제 하이퍼파라미터를 감소시켜야 함"
      ],
      "metadata": {
        "id": "biQX-WfVJixj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 규제가 있는 모델이 일반적으로 규제가 없는 모델보다 성능이 좋다. 평범한 선형 회귀보다 릿지 회귀가 선호됨.\n",
        "- 라쏘 회귀는 자동으로 특성 선택의 효과를 가지므로 단지 몇 개의 특성만 실제 유용할 것이라고 의심될 때 사용해야 함\n",
        "- 특성 수가 훈련 샘플 수보다 많거나, 특성 몇개가 강하게 연관되어 있을 때는 라쏘보다 엘라스틱넷을 사용"
      ],
      "metadata": {
        "id": "Q2wUdvAdKw8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mga5F0f_MlSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}